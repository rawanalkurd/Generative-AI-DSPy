{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c212e14",
   "metadata": {},
   "source": [
    "# Running LLMs Locally: A Guide to Setting Up Ollama with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffef68",
   "metadata": {},
   "source": [
    "**Rawan Alkurd**  \n",
    "*Senior Data Scientist*  \n",
    "[My GitHub Profile](https://github.com/rawanalkurd)  \n",
    "[My LinkedIn Profile](https://linkedin.com/in/rawan-alkurd/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0cb23",
   "metadata": {},
   "source": [
    "In this blog, we will delve into setting up and running a language model using Ollama locally with Docker. Ollama provides a robust platform for deploying and interacting with large language models (LLMs), making it an excellent tool for developers and researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246f681",
   "metadata": {},
   "source": [
    "## What is Ollama?\n",
    "Ollama is a versatile tool designed for deploying and serving LLMs. It simplifies the process of setting up and managing models, allowing users to focus on leveraging the power of LLMs without the overhead of complex infrastructure management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a321055",
   "metadata": {},
   "source": [
    "## Why Use Ollama?\n",
    "- **Ease of Setup**: Ollama's integration with Docker allows for quick and straightforward deployment.\n",
    "- **Flexibility**: Supports various LLMs, including popular models like Llama2 and Llama3.\n",
    "- **Scalability**: Can be configured to run on both CPU and GPU, catering to different performance needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2997c",
   "metadata": {},
   "source": [
    "## Setting Up an LLM and Serving It Locally Using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d8c18",
   "metadata": {},
   "source": [
    "### Step 1: Download the Official Docker Image of Ollama\n",
    "To get started, you need to download the official Docker image of Ollama.\n",
    "\n",
    "For a CPU-only setup, use the following bash command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575f7d8",
   "metadata": {},
   "source": [
    "For a GPU setup, use this bash command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05b541",
   "metadata": {},
   "source": [
    "### Step 2: Pick the Model\n",
    "Ollama supports a wide variety of LLMs. For this example, we'll use Llama2, but more capable models like Llama3 are also available.\n",
    "\n",
    "Run the Ollama image and specify the model with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker exec -it ollama ollama run llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e0b3d",
   "metadata": {},
   "source": [
    "### Step 3: Set Up an Ollama Class to Interact with the Model\n",
    "To interact with the model locally, we'll set up an Ollama class in Python. Here's the source code implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0835eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class OLLAMA:\n",
    "    def __init__(self, model_name, api_endpoint='http://localhost:11434/api/generate', **kwargs):\n",
    "        self.model_name = model_name\n",
    "        self.api_endpoint = api_endpoint\n",
    "        self.session = requests.Session()\n",
    "        self.kwargs = {\"temperature\": 0.7, \"n\": 1, **kwargs}\n",
    "\n",
    "        print(f\"Initialized OLLAMA with model_name: {model_name}, api_endpoint: {api_endpoint}, kwargs: {self.kwargs}\")\n",
    "\n",
    "    def predict(self, question, **kwargs):\n",
    "        output = \"\"\n",
    "        payload = {'model': self.model_name, 'prompt': question, **self.kwargs, **kwargs}\n",
    "\n",
    "        with self.session.post(self.api_endpoint, json=payload, stream=True) as r:\n",
    "            if r.status_code == 200:\n",
    "                for line in r.iter_lines():\n",
    "                    if line:\n",
    "                        j = json.loads(line.decode('utf-8'))\n",
    "                        output += j.get(\"response\", \"\")\n",
    "                        if j.get(\"done\", True):\n",
    "                            break\n",
    "            else:\n",
    "                print(f\"Error: Received status code {r.status_code}\")\n",
    "\n",
    "        return [output.strip()]\n",
    "\n",
    "    def __call__(self, question, **kwargs):\n",
    "        return self.predict(question, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27310bc",
   "metadata": {},
   "source": [
    "#### Description of the Class Components\n",
    "- **Initialization (`__init__`)**: Sets up the model name, API endpoint, and default parameters for the model.\n",
    "- **Prediction (`predict`)**: Sends a request to the model API and processes the streaming response.\n",
    "- **Callable (`__call__`)**: Allows the instance to be called directly with a question, making it more user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baacd95f",
   "metadata": {},
   "source": [
    "## How to Prompt and Use the Ollama Model in the Docker Image?\n",
    "You can directly prompt the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31488b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2 = OLLAMA('llama2', temperature=0)\n",
    "response = llama2('What is the capital of France?')\n",
    "print(\"llama2 Response: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76674d4a",
   "metadata": {},
   "source": [
    "## How to Use the Local Ollama Model with DSPy?\n",
    "DSPy is a framework designed to optimize language model prompts and weights algorithmically, particularly useful when LMs are utilized multiple times within a pipeline. DSPy separates the flow of your program from the parameters of each step, introducing optimizers that can fine-tune prompts and weights based on desired metrics.\n",
    "\n",
    "Let's have a look at how to use DSPy to interact with the Ollama model in the docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import Predict, context\n",
    "\n",
    "pred_qa = Predict('question -> answer')\n",
    "\n",
    "with context(lm=llama2):\n",
    "    resp = pred_qa(question='What is the capital of France?')\n",
    "    print(\"llama2 Response: \", resp.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92b531",
   "metadata": {},
   "source": [
    "By following these steps, you'll effortlessly set up and run an Ollama model locally using Docker. This approach offers a flexible and scalable solution, empowering you to harness powerful language models in your applications.\n",
    "\n",
    "Stay tuned for more blog posts where we'll explore DSPy in greater depth and uncover its robust features. For more details on DSPy, visit their official documentation: [DSPy Documentation](https://dspy-docs.vercel.app/)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
